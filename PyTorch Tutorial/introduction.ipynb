{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch tensor\n",
    "Tensor is a generalization of matrix (scalar, vector, matrix, array with any dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2.), torch.Size([]))\n",
      "(tensor([-1.0000, -0.8000, -0.6000, -0.4000, -0.2000,  0.0000,  0.2000,  0.4000,\n",
      "         0.6000,  0.8000,  1.0000], dtype=torch.float64), torch.Size([11]))\n",
      "(tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64), torch.Size([3, 3]))\n",
      "(tensor([[[1, 2],\n",
      "         [2, 1]],\n",
      "\n",
      "        [[1, 3],\n",
      "         [1, 1]]]), torch.Size([2, 2, 2]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.) # create torch tensor from a scalar\n",
    "print((x, x.shape))\n",
    "x = np.linspace(-1,1,11)\n",
    "x = torch.from_numpy(x) # Make 1-D tensor with torch tensor x from numpy array\n",
    "print((x, x.shape))\n",
    "x = np.ones((3,3))\n",
    "x = torch.from_numpy(x) # Make matrix with torch tensor x from numpy array\n",
    "print((x, x.shape))\n",
    "x = torch.tensor([[[1,2],[2,1]], \n",
    "                  [[1,3],[1,1]]]) # Make 3-D tensor\n",
    "print((x, x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [2, 1]],\n",
      "\n",
      "        [[1, 3],\n",
      "         [1, 1]]])\n",
      "tensor([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Access the element of a tensor\n",
    "print(x)\n",
    "print(x[1,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor operations and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(3.), tensor(4., requires_grad=True), tensor(4., requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# Create tensors\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad = True)\n",
    "b = torch.tensor(4., requires_grad = True)\n",
    "print((x,w,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do arithmetic operations\n",
    "y = w*x + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # Calculate dy/dz where z is the variable of the right hand side which require gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Print the partial derivative\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad) # x\n",
    "print('dy/db:', b.grad) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.,2.]], requires_grad = True)\n",
    "W = torch.tensor([[1.], [2.]], requires_grad = True)\n",
    "y = X @ W # Matrix multiplication as a dot product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.]], grad_fn=<MmBackward>)\n",
      "tensor([[1., 2.]])\n",
      "tensor([[1.],\n",
      "        [2.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(y)\n",
    "print(X.grad)\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient is the center of optimization that pytorch is using (gradient descent)\n",
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9.81311691 -18.5271624 ]\n",
      " [-10.98969145 -20.05211175]\n",
      " [ -9.14606618 -16.61849993]\n",
      " [-10.77816658 -20.45083139]\n",
      " [ -9.06547887 -16.80165087]\n",
      " [-10.16426813 -18.30823122]\n",
      " [ -9.86121886 -18.35006749]\n",
      " [-11.24394795 -20.73336335]\n",
      " [ -9.95732581 -18.83926544]\n",
      " [ -7.50887554 -14.04143122]\n",
      " [-10.15200913 -19.14675185]\n",
      " [ -7.76334914 -14.27469688]\n",
      " [ -8.90596237 -17.028856  ]\n",
      " [ -9.80758717 -18.12075408]\n",
      " [ -9.44489437 -17.5416979 ]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(15,3)*5.-10.\n",
    "W = np.random.rand(3,2)\n",
    "b = np.random.rand(1,2)\n",
    "y = X.dot(W) + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 30.523733731698492\n",
      "40 16.01621796829058\n",
      "60 8.473183075897632\n",
      "80 4.550780608041257\n",
      "100 2.5106436288923266\n"
     ]
    }
   ],
   "source": [
    "W_ = torch.tensor(np.random.rand(3,2), requires_grad=True)\n",
    "b_ = torch.tensor(np.random.rand(1,2), requires_grad=True)\n",
    "\n",
    "def model(X):\n",
    "    return X@W_+b_\n",
    "\n",
    "def loss(y1,y2): \n",
    "    dy = y1-y2\n",
    "    return torch.sum(dy*dy)/dy.numel() # MSE\n",
    "\n",
    "lr = 1e-4 # learning rate/step size\n",
    "for epoch in range(100):\n",
    "    preds = model(X)\n",
    "    ls = loss(y, preds)\n",
    "    # Gradient descent\n",
    "    ls.backward() # Calculate the gradient\n",
    "    with torch.no_grad(): \n",
    "        W_ += -lr*W_.grad\n",
    "        b_ += -lr*b_.grad\n",
    "        # Reset the gradient\n",
    "        W_.grad.zero_()\n",
    "        b_.grad.zero_()\n",
    "    \"\"\"\n",
    "     torch.no_grad() is used to not keeping track or modified the gradient while updating the weight and biases\n",
    "     because keeping track those gradient require some computational time or space\n",
    "    \"\"\"\n",
    "    if (epoch+1)%20 == 0:\n",
    "        print(epoch+1, ls.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -9.8131, -18.5272],\n",
      "        [-10.9897, -20.0521],\n",
      "        [ -9.1461, -16.6185],\n",
      "        [-10.7782, -20.4508],\n",
      "        [ -9.0655, -16.8017],\n",
      "        [-10.1643, -18.3082],\n",
      "        [ -9.8612, -18.3501],\n",
      "        [-11.2439, -20.7334],\n",
      "        [ -9.9573, -18.8393],\n",
      "        [ -7.5089, -14.0414],\n",
      "        [-10.1520, -19.1468],\n",
      "        [ -7.7633, -14.2747],\n",
      "        [ -8.9060, -17.0289],\n",
      "        [ -9.8076, -18.1208],\n",
      "        [ -9.4449, -17.5417]], dtype=torch.float64)\n",
      "tensor([[-10.6779, -17.2290],\n",
      "        [-13.0623, -17.8170],\n",
      "        [-10.5860, -14.9767],\n",
      "        [-11.4022, -19.2356],\n",
      "        [-11.2496, -14.6598],\n",
      "        [-12.5999, -15.8934],\n",
      "        [-11.0354, -16.8328],\n",
      "        [-12.6438, -18.9398],\n",
      "        [-10.8120, -17.5360],\n",
      "        [ -8.6823, -12.7288],\n",
      "        [-10.4567, -18.2033],\n",
      "        [ -9.0870, -12.8346],\n",
      "        [ -9.5914, -15.9341],\n",
      "        [-12.0468, -15.8763],\n",
      "        [-10.4277, -16.1913]], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5106436288923266"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y)\n",
    "print(model(X))\n",
    "ls.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using pytorch built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.45113904  -9.05751699]\n",
      " [-13.15492089  -9.60054367]\n",
      " [-14.22357636 -10.62098673]\n",
      " [-14.72208304 -11.20438794]\n",
      " [-13.37038735 -10.11513412]\n",
      " [-11.99300343  -9.29059384]\n",
      " [-13.29069623  -9.58803233]\n",
      " [-11.69754218  -8.64987671]\n",
      " [-14.56971811 -11.09083115]\n",
      " [-11.66959504  -8.61410047]\n",
      " [-13.78350871 -10.52888819]\n",
      " [-12.80908478  -9.72105798]\n",
      " [-15.8135426  -11.55630361]\n",
      " [-11.24846827  -8.77847353]\n",
      " [-13.88392146 -10.46990163]\n",
      " [-14.13073315 -10.80451767]\n",
      " [-13.07002796  -9.73171972]\n",
      " [-10.76301442  -8.0906776 ]\n",
      " [-15.16105659 -11.19803471]\n",
      " [-13.49421406  -9.98918868]\n",
      " [-11.91519247  -9.3498255 ]\n",
      " [-14.14974331 -10.39512115]\n",
      " [-14.81350529 -11.3575038 ]\n",
      " [-13.28049958 -10.28376555]\n",
      " [-13.08133812  -9.44905192]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(25,3)*5.-10.\n",
    "W = np.random.rand(3,2)\n",
    "b = np.random.rand(1,2)\n",
    "y = X.dot(W) + b\n",
    "print(y)\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in Pytorch.\n",
    "\n",
    "TensorDataset allows us to access a small section/batches of the training data using array notation, it returns tuple of inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.0127, -5.2243, -9.1137],\n",
       "         [-8.9919, -6.6046, -5.1379],\n",
       "         [-8.9444, -8.2487, -5.8085]]), tensor([[-12.4511,  -9.0575],\n",
       "         [-13.1549,  -9.6005],\n",
       "         [-14.2236, -10.6210]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(X, y)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "DataLoader is used to split the data into batches, and use in for-in loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.2482, -8.6836, -6.9292],\n",
      "        [-7.8913, -9.3104, -7.0945],\n",
      "        [-5.8499, -5.9631, -7.5039],\n",
      "        [-8.9810, -6.8539, -9.4173],\n",
      "        [-5.5059, -9.5509, -5.8158]])\n",
      "tensor([[-11.9930,  -9.2906],\n",
      "        [-14.5697, -11.0908],\n",
      "        [-11.6696,  -8.6141],\n",
      "        [-15.8135, -11.5563],\n",
      "        [-11.9152,  -9.3498]])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BE0C109688>\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "\n",
    "print(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Linear\n",
    "instead of initializing weights and bias manually, we can define the model using nn.Linear class from pyTorch, which does it automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0943, -0.1763,  0.1296],\n",
      "        [ 0.1489, -0.4982, -0.0122]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3689, 0.3945], requires_grad=True)\n",
      "<generator object Module.parameters at 0x000001BE0C122CC8>\n",
      "Linear(in_features=3, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(3, 2) # 3 input 2 output\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "print(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Optimizer is used to updating the parameter (updating, reset the gradient).\n",
    "optim.SGD stands for stochastic gradient descent. It's called stochastic because samples are selected in shuffled batches instead of as a single group (common Gradient Descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train with batches of data\n",
    "        ls = 0\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb) # Generate Prediction\n",
    "            loss = loss_fn(pred, yb) # Calculate loss\n",
    "            loss.backward() # Calculate the gradient\n",
    "            opt.step() # Update parameters\n",
    "            opt.zero_grad() # Reset gradients to zero\n",
    "            ls += loss.item()\n",
    "        # Print the progress\n",
    "        if (epoch+1)%10==0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.\n",
    "                 format(epoch+1, num_epochs, ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.9186\n",
      "Epoch [20/200], Loss: 0.7269\n",
      "Epoch [30/200], Loss: 0.5939\n",
      "Epoch [40/200], Loss: 0.4876\n",
      "Epoch [50/200], Loss: 0.3864\n",
      "Epoch [60/200], Loss: 0.3131\n",
      "Epoch [70/200], Loss: 0.2534\n",
      "Epoch [80/200], Loss: 0.2056\n",
      "Epoch [90/200], Loss: 0.1653\n",
      "Epoch [100/200], Loss: 0.1328\n",
      "Epoch [110/200], Loss: 0.1092\n",
      "Epoch [120/200], Loss: 0.0898\n",
      "Epoch [130/200], Loss: 0.0719\n",
      "Epoch [140/200], Loss: 0.0576\n",
      "Epoch [150/200], Loss: 0.0467\n",
      "Epoch [160/200], Loss: 0.0383\n",
      "Epoch [170/200], Loss: 0.0314\n",
      "Epoch [180/200], Loss: 0.0260\n",
      "Epoch [190/200], Loss: 0.0207\n",
      "Epoch [200/200], Loss: 0.0173\n"
     ]
    }
   ],
   "source": [
    "fit(200, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.4874,  -9.1572],\n",
       "        [-13.0716,  -9.6857],\n",
       "        [-14.1625, -10.6280],\n",
       "        [-14.7496, -11.1433],\n",
       "        [-13.4281, -10.0912],\n",
       "        [-12.0316,  -9.2135],\n",
       "        [-13.2360,  -9.7086],\n",
       "        [-11.7539,  -8.7076],\n",
       "        [-14.5587, -11.0299],\n",
       "        [-11.6877,  -8.6758],\n",
       "        [-13.7884, -10.4670],\n",
       "        [-12.8461,  -9.6933],\n",
       "        [-15.8017, -11.6306],\n",
       "        [-11.2511,  -8.6928],\n",
       "        [-13.8941, -10.4494],\n",
       "        [-14.1641, -10.7354],\n",
       "        [-13.0304,  -9.7576],\n",
       "        [-10.7886,  -8.1118],\n",
       "        [-15.0865, -11.2356],\n",
       "        [-13.4605, -10.0304],\n",
       "        [-11.9356,  -9.2360],\n",
       "        [-14.1572, -10.4582],\n",
       "        [-14.8346, -11.2689],\n",
       "        [-13.2870, -10.1851],\n",
       "        [-13.0655,  -9.5670]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols = 2)\n",
    "ax[0].scatter(pred[:,0].detach().numpy(), y[:, 0].detach().numpy())\n",
    "ax[1].scatter(pred[:,1].detach().numpy(), y[:, 1].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
